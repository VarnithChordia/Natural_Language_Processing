{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging as classification\n",
    "\n",
    "Matthew Stone   \n",
    "CS 533    \n",
    "Initial version, Spring 2017.  Updated Spring 2018.\n",
    "\n",
    "This notebook shows how to write a POS classifier (or in general any sequence classifier) by analyzing windows in a string using features.  It assumes that you have the `vocabulary.py` definitions of the vocabulary class, the `squtils` definitions for working conveniently with tagged sequence data, and the `GloVe` 6B 50-dimensional word embeddings available in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "import vocabulary\n",
    "import squtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the definitions for the machine learning problem that we'll be working with.\n",
    "\n",
    "Note that things are slightly more complicated here in terms of accessing the data: we need to have a generator that yields sequences, corresponding to each sentence (because we are going to have to put in START and END tags to model contexts), and we expect that this data will actually have (word, tag) items in it fundamentally.  Since generators are basically \"running functions\", we need to define our access to the data via a call that gives us a new way to run through the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_file, vocab_file_type = \"brown-vocab.pkl\", \"pickle\"\n",
    "\n",
    "embedding_file, embedding_dimensions, embedding_cache = \"D:/Rutgers/4th-Semester/Natural_Language_Processing/glove.6B/glove.6B.50d.txt\", 50, \"brown-embedding.npy\"\n",
    "\n",
    "def mk_data_generator(subset='all') :\n",
    "    if subset == 'dev' :\n",
    "        start = 0\n",
    "        stop = 123\n",
    "    elif subset == 'test' :\n",
    "        start = 123\n",
    "        stop = 623\n",
    "    elif subset == 'train' :\n",
    "        start = 623\n",
    "        stop = 4623\n",
    "    else :\n",
    "        start = 0\n",
    "        stop = 4623\n",
    "    return itertools.islice(nltk.corpus.brown.tagged_sents(categories='news',tagset='universal'),\n",
    "                            start, stop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern below is familiar from last time: We're going to want to have a set of word embeddings to work with as features, to be able to generalize sparse data across items.  The code below loads in a targeted subset of the 50-dimensional GloVe embeddings, and maintains an index between rows in the resulting matrix and specific English words.\n",
    "\n",
    "The code gives a sense of how many words are missing from GloVe (these are likely to be proper names and the like)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "made_vocabulary = False\n",
    "if made_vocabulary :\n",
    "    words = vocabulary.Vocabulary.load(vocab_file, file_type=vocab_file_type)\n",
    "else: \n",
    "    tokens = (w.lower() for (w,t) in itertools.chain.from_iterable(mk_data_generator()))\n",
    "    words = vocabulary.Vocabulary.from_iterable(tokens, file_type=vocab_file_type)\n",
    "    words.save(vocab_file)\n",
    "words.stop_growth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282 words were not in glove\n"
     ]
    }
   ],
   "source": [
    "made_embedding = False\n",
    "if made_embedding :\n",
    "    e = squtils.load_dense_array(embedding_cache)\n",
    "else: \n",
    "    e = squtils.build_dense_embedding(words, embedding_file, embedding_dimensions)\n",
    "    squtils.save_dense_array(embedding_cache, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to be able to represent each window using appropriate features.  The code below uses the 50 dimensional word embedding at each position, the four character prefixes and suffixes at each position, and the word identity at each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mkf(features, name, fl) :\n",
    "    r = features.add(name)\n",
    "    if r :\n",
    "        fl.append(r)\n",
    "    \n",
    "def word_feature_columns(features, code, item) :\n",
    "    f = []\n",
    "    for i in range(0,50) :\n",
    "        mkf(features, \"{}:e{}\".format(code, i), f)\n",
    "    mkf(features, \"{}:w_{}\".format(code, item), f)\n",
    "    for i in range(1,4) :\n",
    "        mkf(features, \"{}:{}_{}\".format(code, i, item[-i:]), f)\n",
    "    for i in range(1,4) :\n",
    "        mkf(features, \"{}:{}{}_\".format(code, i, item[0:i]), f)\n",
    "    return f\n",
    "\n",
    "def word_feature_values(embeddings, vocab, item, f) :\n",
    "    values = np.zeros(len(f))\n",
    "    r = vocab.add(item) \n",
    "    if r: \n",
    "        values[:50] = embeddings[r]\n",
    "    values[50:] = np.ones(len(f)-50)\n",
    "    return values\n",
    "\n",
    "def fivegram_features(features, embeddings, vocab, cxt) :\n",
    "    (n2b, n1b, t, n1a, n2a) = cxt\n",
    "    f2b = word_feature_columns(features, \"w_t\", n2b)\n",
    "    v2b = word_feature_values(embeddings, vocab, n2b, f2b)\n",
    "    f1b = word_feature_columns(features, \"wt\", n1b)\n",
    "    v1b = word_feature_values(embeddings, vocab, n1b, f1b)\n",
    "    ft = word_feature_columns(features, \"t\", t)\n",
    "    vt = word_feature_values(embeddings, vocab, t, ft)\n",
    "    f1a = word_feature_columns(features, \"tw\", n1a)\n",
    "    v1a = word_feature_values(embeddings, vocab, n1a, f1a)\n",
    "    f2a = word_feature_columns(features, \"t_w\", n2a)\n",
    "    v2a = word_feature_values(embeddings, vocab, n2a, f2a)\n",
    "    return np.concatenate([f2b, f1b, ft, f1a, f2a]), np.concatenate([v2b, v1b, vt, v1a, v2a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = vocabulary.Vocabulary()\n",
    "\n",
    "def mk_encoder(features, embeddings, vocab) :\n",
    "    def encode(cxt) :\n",
    "        return fivegram_features(features, embeddings, vocab, cxt)\n",
    "\n",
    "    return encode\n",
    "\n",
    "feature_encoder = mk_encoder(features, e, words)\n",
    "def size_f() : return len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the training data and fix the features we're going to be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtr, ytr = squtils.mk_tagging_data(feature_encoder, size_f, \n",
    "                                   mk_data_generator(subset='train'))\n",
    "features.stop_growth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<86156x88853 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 24468304 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a classifier as always"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='log', max_iter=5, n_iter=None,\n",
       "       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,\n",
       "       shuffle=True, tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = \\\n",
    "sklearn.linear_model.SGDClassifier(loss=\"log\",\n",
    "                                   penalty=\"elasticnet\",\n",
    "                                   max_iter=5)\n",
    "\n",
    "classifier.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want to visualize the action of the classifier, we need some help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the development data and evaluate its classification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9643721895537876\n"
     ]
    }
   ],
   "source": [
    "Xd, yd = squtils.mk_tagging_data(feature_encoder, size_f, \n",
    "                                 mk_data_generator(subset='dev'))\n",
    "pd = classifier.predict(Xd)\n",
    "print(sklearn.metrics.accuracy_score(yd, pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with the tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x1c7c3371788>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squtils.test_tagger(feature_encoder, size_f, classifier,\n",
    "                    \"they refuse to permit us to obtain the refuse permit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
